% !TeX document-id = {0be8c18c-9430-4e9a-bdd9-12beadebfebc}
% !TeX TXS-program:bibliography = txs:///biber
\documentclass[11pt]{beamer}
\uselanguage{portuguese}
\languagepath{portuguese}
\deftranslation[to=portuguese]{Theorem}{Teorema}
\deftranslation[to=portuguese]{theorem}{teorema}
\deftranslation[to=portuguese]{Example}{Exemplo}
\deftranslation[to=portuguese]{example}{exemplo}
\deftranslation[to=portuguese]{Lemma}{Lema}
\deftranslation[to=portuguese]{lemma}{Lema}
\deftranslation[to=portuguese]{Corollary}{Corolário}
\deftranslation[to=portuguese]{corollary}{corolário}
%\deftranslation[to=portuguese]{and}{e}

\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{color}
%\usepackage{appendixnumberbeamer}

\newenvironment{transitionframe}{
	\setbeamercolor{background canvas}{bg=yellow}
	\begin{frame}}{
	\end{frame}
}
\usetheme{default}
\usefonttheme{structuresmallcapsserif}

%% I use a beige off white for my background
\definecolor{MyBackground}{RGB}{255,253,218}
\useinnertheme[shadow]{rounded}
\setbeamercolor{block title}{bg=MyBackground}
\setbeamercolor{block body}{bg=MyBackground}
\setbeamercolor{example title}{bg=MyBackground}
\setbeamercolor{example body}{bg=MyBackground}


\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\purple}[1]{\textcolor{purple}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\setbeamertemplate{navigation symbols}{}
%\setbeamertemplate{page number in head/foot}[appendixframenumber]

%\usepackage{graphics}
\usepackage{graphicx}

\definecolor{blue_emph}{RGB}{0,114,178}
\definecolor{red}{RGB}{213,94,0}
\definecolor{yellow}{RGB}{240,228,66}
\definecolor{green}{RGB}{0,158,115}
\definecolor{purple}{RGB}{204,121,167}
\definecolor{orange}{RGB}{230,159,0}
\definecolor{lightblue}{RGB}{86,180,233}

%\setbeamercolor{frametitle}{fg=blue}
%\setbeamercolor{title}{fg=blue}
\setbeamertemplate{footline}[frame number]
\setbeamertemplate{navigation symbols}{} 
\setbeamertemplate{itemize items}{-}
%\setbeamercolor{itemize item}{fg=blue}
%\setbeamercolor{itemize subitem}{fg=blue}
\setbeamertemplate{enumerate items}[default]
%\setbeamercolor{enumerate subitem}{fg=blue}
\setbeamercolor{button}{bg=MyBackground,fg=blue}
\usefonttheme{structuresmallcapsserif}

%\setbeamercolor{section in toc}{fg=blue}
%\setbeamercolor{subsection in toc}{fg=red}
\setbeamersize{text margin left=1em,text margin right=1em} 


\usepackage{appendixnumberbeamer}

\usepackage[
backend=biber,
style=authoryear,
natbib=true
]{biblatex}
\addbibresource{bib_aula.bib}

\newenvironment{wideitemize}{\itemize\addtolength{\itemsep}{10pt}}{\enditemize}
\newenvironment{wideenumerate}{\enumerate\addtolength{\itemsep}{10pt}}{\endenumerate}
\newenvironment{halfwideitemize}{\itemize\addtolength{\itemsep}{0.5em}}{\enditemize}
\newenvironment{halfwideenumerate}{\enumerate\addtolength{\itemsep}{0.5em}}{\endenumerate}

\author{Luis Antonio Fantozzi Alvarez}
\title{Probabilidade Estatística}
\subtitle{Princípios de Redução de Dados e Elementos da Teoria da Decisão Estatística}
%\logo{aa}


\date{Universidade de São Paulo}
%\subject{Statistics}
%\setbeamercovered{transparent}


% If you like road maps, rather than having clutter at the top, have a roadmap show up at the end of each section 
% (and after your introduction)
% Uncomment this is if you want the roadmap!
% \AtBeginSection[]
%{
	%   \begin{frame}
		%       \frametitle{Roadmap of Talk}
		%       \tableofcontents[currentsection]
		%   \end{frame}
	%}




\begin{document}
	
	\begin{frame}[plain]
		\maketitle
	\end{frame}
	

		\begin{transitionframe}
		\centering 
		\Huge{O Problema de Inferência Estatística}
	\end{transitionframe}
	
	

	
	
	\begin{frame}{Ambiente}
		\begin{halfwideitemize}
			\item Nosso ponto de partida é um espaço mensurável $(\Omega,\Sigma)$.
			\begin{halfwideitemize}
				\item $\Omega$ é o espaço amostral.
				\item $\Sigma$ é o espaço de eventos aos quais podemos atribuir probabilidades ($\sigma$-álgebra).
			\end{halfwideitemize}
			\item Pesquisador observa uma amostra, dada pela variável aleatória $\boldsymbol{X} \coloneqq \operatorname{Id}_{\Omega}$, cuja lei é dada por uma probabilidade $P$ sobre $(\Omega, \Sigma)$ {\color{blue}desconhecida}.
			\item O pesquisador postula uma família de leis de probabilidade $\mathcal{P} = \{P_\theta : \theta \in \Theta\}$ {\color{blue}candidatas a terem gerado a amostra}.
			\begin{halfwideitemize}
				\item $\Theta$ é o {\color{blue}espaço de parâmetros} que indexa a família.
				\item À família $\mathcal{P}$ damos o nome de {\color{blue} modelo}.
			\end{halfwideitemize}
					\item Dizemos que um modelo está {\color{blue}bem-especificado} se $\exists  \theta_0 \in \Theta$, $P = P_{\theta_0}$.
					\item {\color{red}Notação:} $\mathbb{E}_{\theta}$ denota a expectativa de uma variável aleatória com domínio em $(\Omega, \Sigma)$ com respeito a $P_{\theta}$.
		\end{halfwideitemize}
		
	\end{frame}
	
	\begin{frame}{Amostragem aleatória}
	\begin{itemize}
		\item Em diversos contextos, o experimento definido por $(\Omega, \Sigma, P)$ tem a seguinte interpretação: pesquisador possui acesso a uma amostra de $n$ unidades sorteadas ao acaso de uma população de interesse, para os quais ele observa um vetor de $k$ características $X_i$, $i=1,\ldots, n$.
		\item Se população é {\color{blue}grande}, o conceito adequado para modelar o experimento é o de {\color{blue}amostragem aleatória}.
		\begin{halfwideitemize}
			\item \textbf{Amostragem aleatória}: espaço é $(\prod_{i=1}^n (\mathbb{R}^k), \otimes_{i=1}^n \mathcal{B}(\mathbb{R}^k), \otimes_{i=1}^n G)$.
			\begin{itemize}
								\item  $G$ é a lei de probabilidade sobre $(\mathbb{R}^k, \mathcal{B}(\mathbb{R}^k))$ que reflete a distribuição conjunta das características de interesse na população.
				\item Observações $\boldsymbol{X} = (X_1,\ldots, X_n)$ são tais que cada $X_i$ tem a mesma distribuição que $G$, e as observações $X_i$ são independentes  entre si (o valor amostrado na $i$-ésima posição não exibe associação com o valor amostrado em outra posição $j\neq i$)
			\end{itemize}
			\item Quando o experimento é definido por amostragem aleatória, modelo se resume a uma família de distribuições para as características de interesse na população.
			\begin{itemize}
				\item Em outras palavras, modelo é da forma $\mathcal{P} = \{\otimes_{i=1}^n G : G \in \mathcal{G}\}$, onde $\mathcal{G}$ é uma família de distribuições em $(\mathbb{R}^k, \mathcal{B}(\mathbb{R}^k))$
			\end{itemize}

	
		\end{halfwideitemize}
	\end{itemize}
		\end{frame}
		
		\begin{frame}{Exemplo}
		\begin{itemize}
			\item {\color{blue}Exemplo:} pesquisador observa uma variável escalar, e supõe que tenha sido amostrada de uma distribuição normal com média desconhecida e variância unitária.
			\begin{halfwideitemize}
				\item Quem é $\mathcal{P}$?
				\item E se a variância é desconhecida?
				\item E se pesquisador possui $n$ observações independentes?
			\end{halfwideitemize}
		\end{itemize}
	\end{frame}

	
	\begin{frame}{Modelos}
	\begin{itemize}
		\item Um modelo $\mathcal{P}$ é dito de {\color{blue}dimensão finita} quando o espaço de parâmetros pode ser identificado com um subconjunto de $\mathbb{R}^s$, $ s\in \mathbb{N}$.
		\begin{itemize}
			\item Amostragem aleatória de uma população normal com média e variância desconhecidas define um modelo cujo espaço de parâmetros é da forma $\mathbb{R}\times \mathbb{R}_+$.
		\end{itemize}
				\item Um modelo $\mathcal{P}$ é dito de {\color{blue}dimensão infinita} quando ele não tem dimensão finita.
		\begin{itemize}
			\item Amostragem aleatória de uma população cuja distribuição admite uma densidade com respeito à medida de Lebesgue simétrica em relação à média define um modelo de dimensão infinita.
		\end{itemize}
		
	\end{itemize}
	\end{frame}
	\begin{frame}{Definição de estatística}
		\begin{halfwideitemize}
			
			\item Uma {\color{blue}estatística} é uma transformação mensurável da amostra, i.e. uma função $T:\Omega \mapsto \mathcal{T}$ em que $T^{-1}(B) \in \Sigma$,  para todo  $B \in \mathcal{S}$, onde $\mathcal{S}$ é uma $\sigma$-álgebra sobre o espaço $\mathcal{T}$.
			\begin{itemize}
				\item Nesse caso $T(\boldsymbol{X})$ define uma variável aleatória com valores em $\mathcal{T}$.
			\end{itemize}
		
		\end{halfwideitemize}
	\end{frame}
	
	\begin{frame}{Família dominada}
		\begin{itemize}
			\item No que segue, supomso que existe uma medida $\mu$ $\sigma$-finita tal que todo elemento $P_\theta \in \mathcal{P}$ admite densidade $p_\theta$ com respeito a $\mathcal{\mu}$ (nesse caso, dizemos que a família $\mathcal{P}$ é \emph{dominada}).
			\begin{halfwideitemize}
				\item Para famílias de distribuições ``discretas'' sobre o espaço $(\Omega,2^\Omega)$, com $\Omega$ é enumerável, $\mu$ é a medida de contagem e $p_\theta$ são as f.m.p. definidas por $p_{\theta}(x) = P_{\theta}(\{x\})$ para todo $x \in \Omega$.
				\item Para famílias de distribuições ``contínuas'' sobre $(\mathbb{R}^s, \mathcal{B}(\mathbb{R}^s))$, $\mathcal{\mu}$ é a medida de Lebesgue sobre $\mathcal{B}(\mathbb{R}^s)$ e $p_\theta$ são as f.d.p.
			\end{halfwideitemize}

	
			\end{itemize}
	\end{frame}
	\begin{frame}{Condição técnica}
		\begin{itemize}
		 	\item No que segue, vamos supor também que, para qualquer estatística, as distribuições condicionais de $\boldsymbol{X}|T(\boldsymbol{X})$ sob cada $P_\theta$ admitem uma  {\color{blue}probabilidade condicional regular} $(A, t) \mapsto {P}_{\theta}[\boldsymbol{X}\in A|T(\boldsymbol{X}) = t]$.
			\begin{halfwideitemize}
				\item Para cada $A \in \Sigma$, $\omega \mapsto {P}_{\theta}[\boldsymbol{X}\in A|T(\boldsymbol{X}) = T(\omega)]$ é uma versão da probabilidade condicional ${P}_{\theta}[A|\sigma({T}(\boldsymbol{X}))]$; e para cada $\omega \in \Omega$, $A \mapsto {P}_{\theta}[A|T(\boldsymbol{X}) = T(\omega)]$ é uma medida de probabilidade sobre $\Sigma$.
				\item Essa condição limita a complexidade de $(\Omega, \Sigma)$.
				\item Na maior parte dos casos práticos é satisfeita.
				\item Condições suficientes em \citet{Durrett2019} (fora do escopo do curso).
			\end{halfwideitemize}
			\item Exemplo: no caso discreto, podemos tomar
			${P}_{\theta}[\boldsymbol{X} \in A|T(\boldsymbol{X}) = t] = \sum_{a \in A} {P}_{\theta}[\{a\}|T(\boldsymbol{X}) = t]$, onde 
		\end{itemize}
		\vspace{0.5em}
			$${P}_{\theta}[\{a\}|T(\boldsymbol{X}) = t] = \begin{cases}
			\frac{P_\theta(\{a\})}{{P}_\theta[\{T(\boldsymbol{X}) = t\}]},&  \text{se} T(a) = t \text{ e }  P_\theta[\{T(\boldsymbol{X}) = t\}] > 0\\
			0, & \text{se} T(a) \neq t \text{ e }  P_\theta[\{T(\boldsymbol{X}) = t\}] > 0 \\
			P_\theta(\{a\}), & \text{se }  P_\theta[\{T(\boldsymbol{X}) = t\}] = 0
		\end{cases}$$
	\end{frame}
		\begin{transitionframe}
		\centering 
		\Huge{Suficiência Estatística}
	\end{transitionframe}
	
	
	\begin{frame}{Estatística suficiente}
		\begin{halfwideitemize}
			\item Uma estatística é tão somente uma transformação (mensurável) da amostra, isto é, uma função $T(\boldsymbol{X})$ tal que $T^{-1}(A) \in \Sigma$ para todo $A$ na $\sigma$-álgebra do contradomínio.
			\begin{block}{Definição}
				Uma estatística $T$ é dita suficiente para $\theta$ se a distribuição condicional de $\boldsymbol{X}|T(\boldsymbol{X})$ não depende de $\theta$, isto é, se existe $H$ tal que:
				
				\begin{equation*}
					P_\theta[\boldsymbol{X} \in A|T(\boldsymbol{X})] = H(A|T(\boldsymbol{X})), \quad \forall \theta \in \Theta, A \in \Sigma \, .
				\end{equation*}
				
			\end{block}
			\item Uma vez que conhecemos $T(\boldsymbol{X})$, não há mais informação adicional sobre $\theta$ na amostra.
			\item 
		\end{halfwideitemize}
		
	\end{frame}
	
	\begin{frame}{Exemplo}
		Suponha que a amostra consista de duas Bernoullis independentes e identicamente distribuídas, com parâmetro $\theta \in (0,1)$ desconhecido. Neste caso, a f.m.p. é:
		
		$$P_\theta[X_1 = x, X_2 = y] = \theta^x (1-\theta)^{1-x} \theta^y (1-\theta)^{1-y},\, \quad \forall x,y \in \{0,1\},$$
		de onde segue que, para todo $t \in \{0,1/2,1\}$.
		\begin{equation*}
			\begin{aligned}         
				P_\theta[X_1 = x, X_2 = y| X_1 +  X_2 =  2 t] = \frac{P_\theta[X_1 = x, X_2 = y, X_1 +  X_2 =  2 t]}{P_\theta[X_1 +  X_2 =  2 t]} = \\
				= \frac{\theta^{x+y} (1-\theta)^{2-x-y}\mathbf{1}\{x+y = 2t\}}{\binom{2}{2t} \theta^{2t}(1-\theta)^{w-2t}}= \frac{1}{\binom{2}{2t} } \mathbf{1}\{x+y = 2t\} \, ,
			\end{aligned}
		\end{equation*}
		de onde concluímos que $(X_1+X_2)/2$ é suficiente.
	\end{frame}
	\begin{frame}{Teorema da fatoração de Neyman-Fisher}
		\begin{theorem}
			$T(\boldsymbol{X})$ é suficiente para $\theta$ se, e somente se, existem funções $h_\theta$, $\theta \in \Theta$, e $c$ tais que, para todo $\theta \in \Theta$:
			
			$$p_\theta(\boldsymbol{x}) = h_\theta(T(\boldsymbol{x}) )c(\boldsymbol{x}), \quad \mu\text{-q.t.p.}$$
		\end{theorem}
		
		\begin{halfwideitemize}
			\item Critério conveniente para encontrar uma estatística suficiente.
			\begin{halfwideitemize}
				\item No caso discreto, ``$\mu\text{-q.t.p.}$'' pode ser lido como para todo $\boldsymbol{x}$.
				\item No caso contínuo, condição pode ser violada num conjunto de medida de Lebesgue zero (por exemplo, conjuntos enumeráveis de pontos).
			\end{halfwideitemize}
			\item Veremos demonstração no caso discreto.
			\begin{itemize}
				\item Demonstração no caso geral é mais complexo, pois requer resultados adicionais sobre famílias dominadas.
			\end{itemize}
		\end{halfwideitemize}
		
	\end{frame}
	
	\begin{frame}{Teorema da fatoração (caso discreto)}
		No que segue, considere uma família $\mathcal{P}$ sobre o espaçp $(\Omega, 2^\Omega)$, onde $\Omega$ é enumerável.
		\begin{proof}
			\footnotesize
			$\implies$ Suponha que $T(\boldsymbol{X})$ é suficiente. Fixe $x \in \Omega$. Observe que:
			
			\begin{align*}
			p_{\theta}(x) = P_\theta[\{ x\}] =  P_\theta[\{x\} \cap \{T(\boldsymbol{X})= T(x)\}] =\\
			  \mathbb{E}_\theta[P_\theta[\{x\}|T(\boldsymbol{X})] \mathbf{1}_{\{T(\boldsymbol{X}) = T(x)\}}] =\\ \sum_{\omega \in \Omega}  P_\theta[\{x\}|T(\boldsymbol{X}) = T(\omega)] \mathbf{1}_{\{T(\boldsymbol{X}) = T(x)\}}(\omega) p_\theta(\{\omega\}) = \\
			  	 \underbrace{P_\theta[\{x\}|T(\boldsymbol{X})=T(x)]}_{c(x)} \underbrace{\sum_{\omega \in \Omega: T(\omega)= T(x)} p_\theta(\{\omega\})}_{=P_\theta[\{T(\boldsymbol{X}) = T(x)\}] = h_{\theta}(T(x))}
			\end{align*}
			$\impliedby$ Suponha que $p_\theta$ é fatorável. Considere $\tilde{P}[A|T(\boldsymbol{X}) = t] = \frac{\sum_{a\in A}c(a) \mathbf{1}\{T(a) = t\}}{\sum_{\omega \in \Omega : T(\omega) = t} c(\omega)}$ se $\sum_{\omega \in \Omega : T(\omega) = t} c(\omega)>0$ e $S(A)$ do contrário, onde $S$ é uma probabilidade arbitrária sobre $\Omega$. Fácil verificar que $\tilde{P}$ define uma probabilidade condicional regular para todo $\theta$.
		\end{proof}
		
		
		

\end{frame}
	
	\begin{frame}{Exemplo}
		Suponha que o pesquisador observe uma amostra iid $X_1,X_2, \ldots, X_n$ de uma distribuiçao exponencial com parâmetro $\lambda > 0$ desconhecido.  Neste caso, observe que:
		\begin{equation*}
			\begin{aligned}
				p_\lambda(x_1,x_2,\ldots x_n) = \prod_{i=1}^n (\lambda e^{-\lambda x_i} \mathbf{1}\{x_i>0\}) =\\ =  \lambda^n \exp\left(- \lambda \sum_{i=1}^n x_i\right)\mathbf{1}\{\min_i x_i > 0 \} = h_\lambda \left(\sum_{i=1}^n x_i\right) c(\boldsymbol{x}) \, ,
			\end{aligned}
		\end{equation*}
		de onde concluímos que $(X_1+X_2 + \ldots X_n)/n$ é estatística suficiente.
		
	\end{frame}
	
	\begin{frame}{Estatística suficiente minimal}
		\begin{halfwideitemize}
			\item Observe que o conceito de estatística suficiente só nos informa sobre a capacidade de uma transformação em condensar a informação relevante na amostra sobre $\theta$.
			\item Este conceito não versa sobre o ``tamanho'' desta estatística.
			\begin{halfwideitemize}
				\item De fato, a própria amostra, $T(\boldsymbol{X})=\boldsymbol{X}$, é sempre uma estatística suficiente.
			\end{halfwideitemize}
			\item Note que, se $T$ é estatística suficiente e $T = M\circ U$, então $U$ é estatística suficiente (pois $\sigma(T)\subseteq \sigma(U)$).
			\begin{halfwideitemize}
				\item Estatísticas mais ``finas'' que uma estatística suficiente são estatísticas suficientes.
			\end{halfwideitemize}
			\item Na outra direção, podemos pensar na estatística suficiente mais ``grossa'' possível. 
		\end{halfwideitemize}
	\end{frame}
	\begin{frame}{Estatística suficiente minimal (cont.)}
		\begin{block}{Definição}
			Uma estatística $T$ é dita suficiente minimal, se:
			\begin{halfwideenumerate}
				\item $T$ é suficiente.
				\item Para qualquer outra estatística $S$ suficiente, existe $M$ tal que $T = M \circ S$.
			\end{halfwideenumerate}
		\end{block}
		\begin{block}{Lema}
			Considere uma estatística $T$ tal que:
			$$T(\boldsymbol{x}) = T(\boldsymbol{y}) \iff \boldsymbol{y} \in D(\boldsymbol{x}) , $$
			onde
			$$D(\boldsymbol{x}) = \{\boldsymbol{y}:  p_\theta(\boldsymbol{y}) = p_\theta(\boldsymbol{x}) h(x,y) \quad \forall \theta \in \Theta  \text{ e algum } h(x,y) > 0\}\, .$$
			
			Então $T$ é suficiente minimal.
		\end{block}
	\end{frame}
	\begin{frame}{Exemplo}
		Suponha que o espaço amostral é $\mathbb{R}^n_+$. Considere uma amostra aleatória de $U[0,\theta]$, $\theta > 0$.  Neste caso:
		
		$$p_\theta(x_1,\ldots, x_n) = \frac{1}{\theta^n} \mathbf{1}\{\max_i x_i < \theta\} \, .  $$
		
		Pelo critério de fatoração, $X_{(n)} : = \max_i X_i$ é suficiente. Vamos mostrar que é minimal suficiente usando o lema anterior. Observe que $D(\boldsymbol{x}) = \{\boldsymbol{y}: y_{(n)} = x_{(n)}\}$. De fato, se $y_{(n)} \neq x_{(n)}$, ao considerar $\theta' = (x_{(n)}+y_{(n)})/2$, teremos que $0 = p_{\theta'}(\boldsymbol{x}) < p_{\theta'}(\boldsymbol{y})$ ou $0 = p_{\theta'}(\boldsymbol{y}) < p_{\theta'}(\boldsymbol{x})$. Segue do lema anterior que $X_{(n)}$ é minimal.
	\end{frame}
	
	\begin{frame}{Ancilaridade}
		\begin{block}{Definição}
			Uma estatística é dita ancilar para $\theta$ se sua distribuição não depende de $\theta$, i.e., se existe $F$ tal que:
			
			$$P_\theta[T(X) \in A] = F[A], \quad \forall \theta \in \Theta,  A \in \mathcal{S}, $$
			onde $\mathcal{S}$ é $\sigma$-álgebra acoplada ao contradomínio de $T$.
		\end{block}
		\begin{example}
			No modelo linear Gaussiano homocedástico com regressores fixos:
			$$\boldsymbol{y}_{n\times 1} \sim \mathcal{N}(\boldsymbol{Z} \beta, \sigma^2 \mathbb{I}_n) \, ,$$
			onde $\operatorname{posto}(\boldsymbol{Z}) = k$, $\beta \in \mathbb{R}^k$ desconhecido e $\sigma^2 > 0$ conhecido; veremos em Econometria I que a estatística $S = \hat{\boldsymbol{e}}' \hat{\boldsymbol{e}}$, onde $\hat{\boldsymbol{e}} = \boldsymbol{y} - \boldsymbol{Z} \hat{\beta}_{\text{MQO}}$, é ancilar.
		\end{example}
	\end{frame}
	\begin{frame}{Suficiência completa}
		\begin{halfwideitemize}
			\item Gostaríamos de que uma estatística suficiente fosse independente de estatísticas ancilares, visto que essas não nos trazem informação de $\theta$.
			\item Conceito apropriado para isto é o de estatística {\color{blue}completa suficiente}.
			\begin{block}{Definição}
				Uma estatística $T$ é dita completa para $\theta$ se, para qualquer $f$ mensurável com valores reais:
				$$\mathbb{E}_\theta[f(T)] = 0, \forall \theta \in \Theta \implies \mathbb{P}_\theta[f(T) = 0 ] = 1, \forall \theta \in \Theta$$
			\end{block}
			
			\begin{theorem}[Basu]
				Uma estatística {\color{blue}completa suficiente} para  $\theta$ é independente de qualquer estatística ancilar de $\theta$.
			\end{theorem}
			
		\end{halfwideitemize}
		
	\end{frame}
	
	\begin{frame}{Suficiência completa vs. suficiência minimal}
		\begin{theorem}[Bahadur]
			Se $U$ é estatística completa suficiente de dimensão finita, então é suficiente minimal.
		\end{theorem}
		\begin{halfwideitemize}
			\item Recíproca {\color{red}\textbf{não}} é verdadeira: existem estatísticas minimais suficientes de dimensão finita que não são completas (veja \cite{lehmann2006theory}).
		\end{halfwideitemize}
		
	\end{frame}
	
	\begin{transitionframe}
		\centering 
		\Huge{Famílias Exponenciais}
	\end{transitionframe}
	\begin{frame}{Família exponencial}
		\begin{block}{Definição}
			Uma família de distribuições $\mathcal{P} = \{P_\theta : \theta \in \Theta\}$ sobre um espaço mensurável $(\Omega, \Sigma)$ é dita uma família exponencial se:
			\begin{halfwideenumerate}
				\item Existe uma medida $\mu$ tal que cada elemento $P_\theta \in \mathcal{P}$  admite densidade $p_\theta$ com respeito a $\mu$.
				\item Existem $\eta : {\Theta} \mapsto  \mathbb{R}^s$, $T: \Omega \mapsto \mathbb{R}^s$, $B: \Theta \mapsto \mathbb{R}$ e $h: \Omega \mapsto \mathbb{R}_+$ tais que:
				$$p_\theta(\boldsymbol{x}) = \exp(\eta(\theta)'T(\boldsymbol{x}) -B(\theta)) h(\boldsymbol{x})$$
			\end{halfwideenumerate}
		\end{block}
		\begin{halfwideitemize}
			\item Diversas famílias de distribuições conhecidas constituem uma família exponencial: Gamma, Chi-Quadrado, Beta, Normal, Poisson, Negativo-Binomial.
			\item Propriedade útil: se $\mathcal{P}_1,\ldots, \mathcal{P}_n$ são famílias exponenciais, então $\mathcal{P}^n : = \{\otimes_{i=1}^n P_i: P_i \in \mathcal{P}_i\}$ é uma família exponencial.
			\begin{wideitemize}
				\item Consequência: a distribuição amostral de uma amostra aleatória de uma família exponencial também constitui uma família exponencial.
			\end{wideitemize}
		\end{halfwideitemize}
	\end{frame}
	\begin{frame}{Exemplo}
		Sejam $\boldsymbol{X}= (X_1, X_2, \ldots, X_n)'$ uma amostra aleatória de $N(\mu, \sigma^2)$, onde $\mu \in \mathbb{R}$ e $\sigma^2 >0$ são desconhecidos. Neste caso, fazendo $\theta = (\mu, \sigma^2)'$, temos:
		\begin{equation}
			\begin{aligned}
				p_\theta(x_1,x_2,\ldots, x_n) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(\frac{-(x_i-\mu)^2}{2\sigma^2}\right) =  \\ 
				\exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n x_i^2 + \frac{\mu}{\sigma^2} \sum_{i=1}^n x_i - \frac{\mu^2}{2\sigma^2}n\right)\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n \, ,
			\end{aligned}
		\end{equation}
		de onde segue que a família é exponencial com $T(\boldsymbol{x}) = (\sum_{i=1}^n x_i^2, \sum_{i=1}^n x_i)'$ e ${\eta}(\theta) = \left(-\frac{1}{2\sigma^2}, \frac{\mu}{\sigma^2}\right)'$.
	\end{frame}
	
	\begin{frame}{Identificabilidade de famílias de distribuições}
		\begin{block}{Definição}
			Para uma família $\{P_\theta: \theta \in \Theta\}$, dizemos que o parâmetro é identificável se:
			$$\theta = \theta' \iff P_{\theta} = P_{\theta'}$$
		\end{block}
		\begin{halfwideitemize}
			\item Identificação é o requerimento básico para estimação pontual (mais à frente).
			\begin{halfwideitemize}
				\item Requer a existência de um mapa 1-1 entre parâmetro e distribuição amostral.
			\end{halfwideitemize}
			\item No exemplo anterior, $\theta$ é identificado, visto que $f_1(P_\theta) := \mathbb{E}_\theta[X_1] = \mu$ e $f_2(P_\theta) := \mathbb{E}[X_1^2] = \sigma^2 + \mu^2$.
			\item Por outro lado, se a família paramétrica fosse $\{\mathcal{N}( \theta \lor 0, 1): \theta \in \mathbb{R}\}$, $\theta$ não é identificável.
		\end{halfwideitemize}
	\end{frame}
	\begin{frame}{Parametrização natural de família exponencial}
		Note que, da definição de família exponencial:
		
		$$p_\theta(\boldsymbol{x}) = \exp(\eta(\theta)'T(\boldsymbol{x}) -B(\theta)) h(\boldsymbol{x}) \, , \quad  \theta \in \Theta \, ,$$
		segue uma reparametrização {\color{blue}natural}
		$$p_{\boldsymbol{\eta}}(\boldsymbol{x}) = \exp(\eta'T(\boldsymbol{x}) -B^*(\boldsymbol{\eta})) h(\boldsymbol{x}) \, , \quad \boldsymbol{\eta} \in \Xi \, ,$$
		onde $\Xi := \{ \eta(\theta): \theta \in \Theta\} \subseteq \mathbb{R}^s$ é o {\color{blue}espaço paramétrico natural}; e $B^*(\boldsymbol{\eta}) = B(\theta)$ para algum (qualquer) $\theta \in \Theta$ tal que $\eta(\theta) = \boldsymbol{\eta}$
		
		\begin{block}{Definição}
			Uma família exponencial $\{P_\theta: \theta \in \Theta\}$ é dita uma família de posto cheio se sua reparametrização natural é tal que $\eta$ é identificável e $\Xi$ contém uma bola aberta de $\mathbb{R}^s$.
		\end{block}
		\begin{halfwideitemize}
			\item No exemplo de amostra aleatória normal, $\Theta = (-\infty,0) \times \mathbb{R}$, logo a família tem posto cheio.
			\item $\{N(\mu, \mu^2): \mu \in \mathbb{R}\}$ não tem posto cheio (família curvada).
		\end{halfwideitemize}
	\end{frame}
	\begin{frame}{Suficiência em famílias exponenciais}
		\begin{corollary}
			Numa família exponencial, $T(\boldsymbol{X})$ é uma estatística suficiente.
		\end{corollary}
		\begin{corollary}
			Numa família exponencial {\color{blue}de posto cheio}, $T(\boldsymbol{X})$ é uma estatística suficiente minimal.         
		\end{corollary}
		\begin{theorem}
			Numa família exponencial {\color{blue}de posto cheio}, $T(\boldsymbol{X})$ é uma estatística suficiente completa.         
		\end{theorem}
	\end{frame}

	\begin{transitionframe}
		\centering
		\Huge{Elementos de decisão estatística}
	\end{transitionframe}
	
	\begin{frame}{O problema de decisão estatística}
	\begin{itemize}
		\item Considere um experimento estatístico $(\Omega,\Sigma, P)$, e um modelo de leis de probabilidade candidatas para $P$, $\mathcal{P} = \{P:\theta \in \Theta\}$, com $P=P_{\theta_0}$ para algum $\theta_0 \in \Theta$.
		\item Suponha que o pesquisador esteja interessado em realizar inferência sobre um parâmetro  escalar $\psi(\theta_0) \in \mathbb{R}$.
		\begin{itemize}
					\item Após observar $\boldsymbol{X} =\operatorname{Id}_\Omega$, pesquisador deve produzir  estimativa para $\psi(\theta_0)$.
		\end{itemize}
		\item A {\color{blue}perda} que o pesquisador incorre em afirmar um valor $c$ a $\psi(\theta_0)$ quando $\theta_0 = \theta$ é dada por $L(\theta,c)$, com $L: \Theta \times \mathbb{R} \mapsto \mathbb{R}_+$.
		\begin{itemize}
			\item Perda quadrática: $L(\theta,c)= (c-\psi(\theta))^2$.
			\item Perda absoluta: $L(\theta,c) = |c-\psi(\theta)|$.
		\end{itemize}
		\item Uma {\color{blue}regra de decisão estatística} ou estimador é uma transformação mensurável $\delta:\Omega \mapsto \mathbb{R}$.
		\begin{itemize}
			\item Após observar o valor de $\boldsymbol{X}(\omega)$ realizado,  pesquisador reporta a estimativa dada por $\delta(\boldsymbol{X}(\omega))$.
		\end{itemize}
		\item A perda esperada ou risco da regra de decisão $\delta$, quando $\theta_0=\theta$, é dada por:
	$$R(\theta, \delta)=\mathbb{E}_\theta[L(\theta, \delta(\boldsymbol{X}))]$$

	\end{itemize}
	\end{frame}
	


	
	\begin{frame}{Escolhendo uma regra de decisão}
		\begin{itemize}
			\item Dado que desconhecemos o valor verdadeiro de $\theta_0$, uma regra de comunicação ideal $\delta^*$ numa classe de regras candidatas $\Delta$ deveria ser tal que, para todo $\theta \in \Theta$:
			$$R(\theta,\delta^*) = \inf_{\delta \in \Delta}R(\theta,\delta)\, ,$$
			\item Infelizmente, se $\Delta$ é irrestrita, problema não possuirá, no geral, solução.
			\begin{itemize}
				\item Por exemplo, sob perda quadrática, para todo $\theta \in \Theta$, a regra de decisão $\delta(\boldsymbol{X}) = \psi(\theta)$ atinge risco zero sob $\theta_0=\theta$, o que implica que, a não ser em casos extremos em que $\theta \mapsto \psi(\theta)$ é constante ou $\boldsymbol{X}$ é perfeitamente informativo sobre $\psi(\theta)$, a regra ótima $\delta^*$ inexistirá.
			\end{itemize}
			\item Dada a inexistência, no geral, de um $\delta^*$ que minimiza o risco uniformemente sobre $\Theta$ quando $\Delta$ é irrestrito, a literatura estatística propõe dois conjuntos soluções alternativas ao problema
			\begin{enumerate}
				\item Restringir $\Delta$.
				\item Minimizar o pior risco sobre $\Theta$ (estimação minimax) ou um risco médio sobre $\Theta$, onde a média se dá com respeito a uma distribuição sobre $\Theta$ (estimadores de Bayes). 
			\end{enumerate}
		\end{itemize}
	\end{frame}
		\begin{frame}{Restringindo $\Delta$: estimadores não viciados}
		\begin{itemize}
			\item Uma restrição bastante comum à classe de estimadores é de que eles não possuam viés.
			\item Formalmente, um estimador $\delta$ de um parâmetro $\psi(\theta)$ é {\color{blue}não viciado} numa família $\mathcal{P} =\{P_\theta: \theta \in \Theta\}$ de distribuições candidatas se:
			
			$$\mathbb{E}_\theta[\delta(\boldsymbol{X})] = \psi(\theta),\quad \forall \theta \in \Theta\, .$$
			\item Um estimador $\delta$ de $\psi(\theta)$ é dito não viciado de variância uniformemente mínima (numa família $\mathcal{P}$) se:
			\begin{enumerate}
				\item $\delta$ é não viciado.
				\item para qualquer outro estimador $\tilde{\delta}$ de $\psi(\theta)$ não viciado em $\mathcal{P}$:
				$$\mathbb{V}_\theta[\delta(\boldsymbol{X})] \leq \mathbb{V}_\theta[\tilde{\delta}(\boldsymbol{X})], \quad \forall \theta \in \Theta$$
			\end{enumerate}			
			
			
			
		\end{itemize}
	\end{frame}
	
		\begin{frame}{Caracterizando estimadores NVVUM}
		Um estimador $\delta$ é dito de variância finita em $\mathcal{P}$ se $\mathbb{V}_\theta[\delta(\boldsymbol{X})] < \infty$ para todo $\theta \in \Theta$. 
		\begin{theorem}
			Um estimador $\delta$ de variância finita e não viciado para  $\psi(\theta)$ em $\mathcal{P}$ é não viciado de variância uniformemente mínima em $\mathcal{P}$ se, e somente se, para qualquer estimador $U$  de variância finita e não viciado para zero em $\mathcal{P}$:
			
			$$\mathbb{E}_\theta[\delta(\boldsymbol{X}) U(\boldsymbol{X})] = 0\, , \quad \forall \theta \in \Theta\, .$$
		\end{theorem}
		\begin{corollary}
			Se um estimador $\delta$ de $\psi(\theta)$ com variância finita é não viciado com variância uniformemente mínima em $\mathcal{P}$, então ele é único.
		\end{corollary}
	\end{frame}
	
	\begin{frame}{Funções perdas mais gerais}
	\begin{itemize}
		\item Estimadores não viciados de variância uniformemente mínima minimizam o risco, sob perda quadrática, uniformemente na classe de estimadores não viciados.
		\item Será que é possível encontrar estimadores que minimizem o risco uniformemente, na mesma classe, para outras funções perdas?
		\begin{itemize}
			\item Resposta dada pelo teorema abaixo.
		\end{itemize}
		

		
		\begin{theorem}
			Seja $\delta$ um estimador não viciado de um parâmetro $\psi(\theta)$ numa família $\mathcal{P}$, e $T$ uma estatística completa suficiente para $\mathcal{P}$. Então:
			\begin{enumerate}
				\item $\phi(\boldsymbol{X}) = \mathbb{E}[\delta|T(\boldsymbol{X})]$ define um estimador não viciado de $\psi(\theta)$, e o único estimador não viciado de $\psi(\theta)$ que é função de $T$.
				\item $\phi$ minimiza o risco uniformemente na classe de estimadores não viciados de $\psi(\theta)$, para qualquer perda $L(\theta,z)$ convexa no segundo argumento.
				\item Se $\phi$ possui risco finito para algum $\theta$ e a perda é estritamente convexa no segundo argumento, então $\phi$ é o único estimador que minimiza o risco uniformemente  na classe de estimadores não viciados de $\psi(\theta)$.
			\end{enumerate}
		\end{theorem}
	\end{itemize}
	\end{frame}
	
			\begin{frame}{Limite inferior de Crámer-Rao}
		\begin{itemize}
			\item Resultados anteriores nos deram dois métodos para se tentar construir um ENVVUM.
			\begin{itemize}
				\item Encontrar $\delta$ resolvendo o ``sistema de equações'': $\mathbb{E}_\theta[\delta(\boldsymbol{X})]=\psi(\theta)$, $\mathbb{E}_\theta[\delta(\boldsymbol{X}) U(\boldsymbol{X})] = 0$ para todo $\theta$ e $U$ não viciado para zero (de variância finita).
				\item Encontrar um estimador não viciado, uma estatística completa suficiente e calcular a esperança condicional.
			\end{itemize}
			\item Uma pergunta alternativa é: se temos um estimador não viciado, será que existe uma condição {\color{blue}suficiente} simples de ser verificada, que quando garantida implica que estimador é ENVVUM?
			\begin{itemize}
				\item Resposta é afirmativa, e dada pelo limite inferior de Crámer-Rao.
				\item Esse limite nos dá a menor variância que um estimador não viciado de um parâmetro em uma família de dimensão finita pode atingir.
				\item Limite nem sempre é atingível por um estimador, mas, se for o caso, sabemos que ele é ENVVUM.
			\end{itemize}
		\end{itemize}
	\end{frame}
	\begin{frame}{Limite inferior de Crámer-Rao: derivação}
		\begin{itemize}
				\item Seja $\mathcal{P} = \{P_\theta : \theta  \in \Theta\}$ uma família (dominada) de dimensão finita, com $\Theta \subseteq \mathbb{R}^s$.
				\item Suponha que, para todo $\theta \in \operatorname{int}(\Theta)$ e $x \in \Omega$, $\tau \mapsto p_\tau(x)$ e $\tau \mapsto \psi(\tau)$ são continuamente diferenciáveis em $\tau = \theta$.
			\item Considere um estimador $\delta$ não viciado de um parâmetro escalar $\psi(\theta)$.
			\item Tome um ponto $\theta \in \operatorname{int}(\Theta)$.
			
			\item Considere uma direção $\boldsymbol{e} \in \mathbb{R}^s$, $\boldsymbol{e} \neq \boldsymbol{0}$.
			\item Defina a variável aleatória $S_{\theta,\boldsymbol{e}}(x) = \frac{\nabla_{\tau} p_{\theta_0}(x) \cdot \boldsymbol{e}}{p_{\theta}(x)}$.
				\item Observe que, como $\int p_{\tau}(x)\mu(dx)=1$ para todo $\tau$, se pudermos diferenciar em $\tau$ por dentro da integral, segue que $\mathbb{E}_{\theta}[S_{\theta,\boldsymbol{e}}(x)] = 0$ e que $\operatorname{cov}_{\theta}(\delta, S_{\theta,\boldsymbol{e}}) = \int\delta(x)  (\nabla_{\tau} p_{\theta}(x) \cdot \boldsymbol{e})  \mu(dx)$.
				\item Ademais, como $\int \delta(x) p_\tau(x) \mu(dx) =\psi(\tau) $ para todo $\tau$, se pudermos diferenciar por debaixo da integral, segue que $   \operatorname{cov}_{\theta}(\delta, S_{\theta,\boldsymbol{e}}) =\nabla_{\tau}\psi(\theta) \cdot \boldsymbol{e}$
			
		\end{itemize}

		
	
		
	\end{frame}
	
	\begin{frame}{Derivação (cont)}
		\begin{itemize}
			\item Combinando os fatos anteriores com a desigualdade de Cauchy-Schwarz, temos:
			
			$$\frac{(\partial_{\tau}\psi(\theta)\cdot \boldsymbol{e})^2}{ \mathbb{V}_\theta[S_{\theta,\boldsymbol{e}}]} \leq \mathbb{V}_{\theta}[\delta]$$
			onde $\mathbb{V}_\theta[S_{\theta,\boldsymbol{e}}]= \boldsymbol{e}'\mathbb{V}_\theta\left[\partial_\tau \log p_\theta\right]\boldsymbol{e}$, e $\mathbb{V}_\theta\left[\partial_\tau \log p_\theta\right]$ é a matriz de covariância do vetor aleatório $\partial_\tau \log p_\theta = \frac{\partial_\tau p_\theta}{p_\theta}$.
			\item Não é difícil mostrar que, se $\mathbb{V}_\theta\left[\partial_\tau \log p_\theta\right]$ tem posto cheio, então, maximizando com respeito a $\boldsymbol{e}$, chegamos a:
			
				$$ \mathbb{V}_{\theta}[\delta]\geq (\partial_{\tau}\psi(\theta))' I(\theta)^{-1} (\partial_{\tau}\psi(\theta))\, ,$$
				onde $I(\theta) \coloneqq \mathbb{V}_\theta\left[\partial_\tau \log p_\theta\right]$ é conhecida como informação de Fisher.
				\begin{itemize}
					\item Invertibilidade de $I(\theta)$ está relacionada com identificabilidade da família $\{P_\theta\}$.
					\item Sob condições adicionais de diferenciabilidade, fácil ver que $I(\theta) = -\mathbb{E}_\theta[\frac{\partial^2}{\partial \tau\partial\tau'} \log p_\theta]$.
				\end{itemize}
		\end{itemize}
	\end{frame}
	
	\begin{transitionframe}
		\centering
		\Huge{Referências}
	\end{transitionframe}
	\begin{frame}{Referências}
		\printbibliography
	\end{frame}
	
\end{document}

