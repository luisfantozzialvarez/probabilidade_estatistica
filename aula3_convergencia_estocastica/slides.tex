% !TeX document-id = {0be8c18c-9430-4e9a-bdd9-12beadebfebc}
% !TeX TXS-program:bibliography = txs:///biber
\documentclass[11pt]{beamer}
\uselanguage{portuguese}
\languagepath{portuguese}
\deftranslation[to=portuguese]{Theorem}{Teorema}
\deftranslation[to=portuguese]{Definition}{Definição}
\deftranslation[to=portuguese]{theorem}{teorema}
\deftranslation[to=portuguese]{Example}{Exemplo}
\deftranslation[to=portuguese]{example}{exemplo}
\deftranslation[to=portuguese]{Lemma}{Lema}
\deftranslation[to=portuguese]{lemma}{Lema}
\deftranslation[to=portuguese]{Corollary}{Corolário}
\deftranslation[to=portuguese]{corollary}{corolário}
%\deftranslation[to=portuguese]{and}{e}


\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{color}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{subcaption}
%\usepackage{appendixnumberbeamer}

\newenvironment{transitionframe}{
	\setbeamercolor{background canvas}{bg=yellow}
	\begin{frame}}{
	\end{frame}
}
\usetheme{default}
\usefonttheme{structuresmallcapsserif}

%% I use a beige off white for my background
\definecolor{MyBackground}{RGB}{255,253,218}
\useinnertheme[shadow]{rounded}
\setbeamercolor{block title}{bg=MyBackground}
\setbeamercolor{block body}{bg=MyBackground}
\setbeamercolor{example title}{bg=MyBackground}
\setbeamercolor{example body}{bg=MyBackground}


\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\purple}[1]{\textcolor{purple}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\setbeamertemplate{navigation symbols}{}
%\setbeamertemplate{page number in head/foot}[appendixframenumber]

%\usepackage{graphics}
\usepackage{graphicx}

\definecolor{blue_emph}{RGB}{0,114,178}
\definecolor{red}{RGB}{213,94,0}
\definecolor{yellow}{RGB}{240,228,66}
\definecolor{green}{RGB}{0,158,115}
\definecolor{purple}{RGB}{204,121,167}
\definecolor{orange}{RGB}{230,159,0}
\definecolor{lightblue}{RGB}{86,180,233}

%\setbeamercolor{frametitle}{fg=blue}
%\setbeamercolor{title}{fg=blue}
\setbeamertemplate{footline}[frame number]
\setbeamertemplate{navigation symbols}{} 
\setbeamertemplate{itemize items}{-}
%\setbeamercolor{itemize item}{fg=blue}
%\setbeamercolor{itemize subitem}{fg=blue}
\setbeamertemplate{enumerate items}[default]
%\setbeamercolor{enumerate subitem}{fg=blue}
\setbeamercolor{button}{bg=MyBackground,fg=blue}
\usefonttheme{structuresmallcapsserif}

%\setbeamercolor{section in toc}{fg=blue}
%\setbeamercolor{subsection in toc}{fg=red}
\setbeamersize{text margin left=1em,text margin right=1em} 


\usepackage{appendixnumberbeamer}

\usepackage[
backend=biber,
style=authoryear,
natbib=true
]{biblatex}
\addbibresource{../bibliography.bib}

\newenvironment{wideitemize}{\itemize\addtolength{\itemsep}{10pt}}{\enditemize}
\newenvironment{wideenumerate}{\enumerate\addtolength{\itemsep}{10pt}}{\endenumerate}
\newenvironment{halfwideitemize}{\itemize\addtolength{\itemsep}{0.5em}}{\enditemize}
\newenvironment{halfwideenumerate}{\enumerate\addtolength{\itemsep}{0.5em}}{\endenumerate}

\def\signed #1{{\leavevmode\unskip\nobreak\hfil\penalty50\hskip2em
		\hbox{}\nobreak\hfil(#1)%
		\parfillskip=0pt \finalhyphendemerits=0 \endgraf}}

\newsavebox\mybox
\newenvironment{aquote}[1]
{\savebox\mybox{#1}\begin{quote}}
	{\signed{\usebox\mybox}\end{quote}}

\newtheorem{proposition}{Proposição}



\author{Luis A. F. Alvarez}
\title{Probabilidade e Estatística}
\subtitle{Aula 3 -- Convergência estocástica}
%\logo{}
%\institute{}
\date{\today}
%\subject{}
%\setbeamercovered{transparent}

\begin{document}
	
	\maketitle
	
	\begin{frame}{Vetores aleatórios}
		\begin{itemize}
			\item Seja $(\Omega,\Sigma,\mathbb{P})$ um espaço de probabilidade.
			\item Um vetor aleatório $\boldsymbol{Y}: \Omega \mapsto \mathbb{R}^k$ é uma função tal que cada coordenada $\boldsymbol{Y}_l : \Omega \mapsto \mathbb{R}$, $l=1,\ldots, k$, é uma variável aleatória real.
			\item Um vetor aleatório induz uma distribuição de probabilidade sobre $(\mathbb{R}^k,\mathcal{B}(\mathbb{R}^k))$, dada por $\mathbb{P}_{\boldsymbol{Y}}[A] = \mathbb{P}[\boldsymbol{Y}^{-1}(A)]$, $A \in \mathcal{B}(\mathbb{R}^k)$.
			\item Pelo lema do $\pi$-sistema, essa distribuição de probabilidade é carectarizada pela função de distribuição $F_{\boldsymbol{Y}}: \mathbb{R}^k \mapsto [0,1]$, dada por:
			$$F_{\boldsymbol{Y}}(\boldsymbol{c}) \coloneqq \mathbb{P}_{\boldsymbol{Y}}\left[\prod_{l=1}^k (-\infty,\boldsymbol{c}_l]\right]\, , \quad \boldsymbol{c} \in \mathbb{R}^k\, .$$
			
		\end{itemize}

	\end{frame}
	\begin{frame}{Convergência quase-certa}
		\begin{itemize}
			\item  Seja $(\Omega,\Sigma,\mathbb{P})$ um espaço de probabilidade, e $\boldsymbol{Y}_1,\boldsymbol{Y}_2,\ldots$ uma sequência de vetores aleatórios.
			\item Dizemos que $\boldsymbol{Y}_n$ converge quase-certamente para um vetor aleatório $\boldsymbol{Y}$, denotado por $\boldsymbol{Y}_n \overset{\text{q.c.}}{\to} \boldsymbol{Y}$, se:
			
			$$\mathbb{P}[\{\omega: \boldsymbol{Y}_n(\omega) \nrightarrow \boldsymbol{Y}(\omega)\}] = 0\, .$$
			\item Sequência de funções $\boldsymbol{Y}_n$ convergem (ponto a ponto), a não ser num conjunto de pontos de probabilidade zero.
			
			\begin{lemma}
				$\boldsymbol{Y}_n \overset{\text{q.c.}}{\to} \boldsymbol{Y}$ se, e somente se, para todo $\epsilon > 0$:
				
			$$\mathbb{P}\left[\limsup_n \{\omega: \lVert\boldsymbol{Y}_n(\omega)-\boldsymbol{Y}(\omega)\rVert > \epsilon\}\right]=0\, .$$
			\end{lemma} 
		\end{itemize}

	\end{frame}
	
	\begin{frame}{Convergência em probabilidade}
		\begin{itemize}
				\item Dizemos que $\boldsymbol{Y}_n$ converge em probabilidade para um vetor aleatório $\boldsymbol{Y}$, denotado por $\boldsymbol{Y}_n \overset{p}{\to} \boldsymbol{Y}$, se, para todo $\epsilon > 0$:
					$$\lim_{n \to \infty} \mathbb{P}\left[\{\omega: \lVert\boldsymbol{Y}_n(\omega)-\boldsymbol{Y}(\omega)\rVert > \epsilon\}\right]= 0\, .$$
				
		\end{itemize}
		\begin{lemma}
			Se $\boldsymbol{Y}_n \overset{\text{q.c.}}{\to} \boldsymbol{Y}$, então $\boldsymbol{Y}_n \overset{p}{\to} \boldsymbol{Y}$.
		\end{lemma}
	\end{frame}
	
	\begin{frame}{Convergência em distribuição}
			\begin{itemize}
			\item Dizemos que $\boldsymbol{Y}_n$ converge em distribuição para um vetor aleatório $\boldsymbol{Y}$, denotado por $\boldsymbol{Y}_n \overset{d}{\to} \boldsymbol{Y}$, se as funções de distribuição dos $\boldsymbol{Y}_n$, $\{F_{\boldsymbol{Y}_n}\}_{n \in \mathbb{N}}$, convergem para a função de distribuição $F_{\boldsymbol{Y}}$ nos pontos em que  $F_{\boldsymbol{Y}}$ é contínua.
						\begin{itemize}
				\item Isto é, $\lim_n F_{\boldsymbol{Y}_n}(\boldsymbol{c}) = F_{\boldsymbol{Y}}(\boldsymbol{c}) $ para todo $\boldsymbol{c}$ em que $F_{\boldsymbol{Y}}$ é contínua.
						\item Conjunto de pontos em que uma função de distribuição é descontínua é enumerável $\implies$ se há convergência em distribuição, então, para qualquer $\boldsymbol{c} \in \mathbb{R}^k$, é sempre possível encontrar um $\boldsymbol{c}' \geq \boldsymbol{c}$ em que a função de distribuição converge.
					\end{itemize}
					

		\end{itemize}
		\begin{lemma}
			Se $\boldsymbol{Y}_n \overset{p}{\to} \boldsymbol{Y}$, então $\boldsymbol{Y}_n \overset{d}{\to} \boldsymbol{Y}$.
		\end{lemma}
		\begin{lemma}[Trecho do lema Portmanteau]
		$\boldsymbol{Y}_n \overset{d}{\to} \boldsymbol{Y}$ se, e somente se, para qualquer $f: \mathbb{R}^k \mapsto \mathbb{R}$ contínua e limitada.
			$$\mathbb{E}[f(\boldsymbol{Y}_n )] \to \mathbb{E}[f(\boldsymbol{Y} )]\, .$$
		\end{lemma}
	\end{frame}
	
	\begin{frame}{Convergência em $L^p$}
		\begin{itemize}
			\item 		Dizemos que $\boldsymbol{Y}_n$ converge para um vetor aleatório $\boldsymbol{Y}$ na norma $L^p$, denotado por $\boldsymbol{Y}_n \overset{{L}_p}{\to} \boldsymbol{Y}$, se $\lVert  \boldsymbol{Y}_n -\boldsymbol{Y}\rVert_p \to 0$.
			\item Pela desigualdade de Markov, convergência em $L_p$ implica convergência em probabilidade.
			\begin{itemize}
				\item Recíproca não é, no geral, verdadeira.
			\end{itemize} 
		\end{itemize}

	\end{frame}
	
	\begin{frame}{Teorema do mapa contínuo}
	\begin{theorem}
		Seja $(\boldsymbol{X}_n)_n$ uma sequência de vetores aleatórios, $\boldsymbol{X}$ um vetor aleatório, e $f:\mathbb{R}^k \mapsto \mathbb{R}^l$ uma função contínua num conjunto $C$ do domínio tal que:
		$\mathbb{P}[\{\omega:\boldsymbol{X}(\omega)\in C\}]=1$. Então:
		
		\begin{enumerate}
			\item $\boldsymbol{X}_n \overset{\text{q.c.}}{\to} \boldsymbol{X} \implies f(\boldsymbol{X}_n) \overset{\text{q.c.}}{\to} f(\boldsymbol{X} ) $.
			\item  $\boldsymbol{X}_n \overset{\text{p}}{\to} \boldsymbol{X} \implies f(\boldsymbol{X}_n) \overset{\text{p}}{\to} f(\boldsymbol{X} ) $.
			 \item $\boldsymbol{X}_n \overset{\text{d}}{\to} \boldsymbol{X} \implies f(\boldsymbol{X}_n) \overset{\text{d}}{\to} f(\boldsymbol{X} ) $.
		\end{enumerate}
	\end{theorem}
	\begin{itemize}
		\item Modos de convergência quase certa, em probabilidade e distribuição são preservados por transformações contínuas.
	\end{itemize}
	\end{frame}
	
	
	\begin{frame}{Resultados adicionais e lema de Slutsky}
		\begin{lemma}
			\begin{enumerate}
				\item Se $\boldsymbol{X}_n \overset{\text{d}}{\to} \boldsymbol{X}$ e $\lVert \boldsymbol{X}_n - \boldsymbol{Y}_n \rVert\overset{p}{\to}0$, então $\boldsymbol{Y}_n \overset{d}{\to} \boldsymbol{X}$.
				\item Se $\boldsymbol{X}_n \overset{\text{d}}{\to} \boldsymbol{X}$ e $\boldsymbol{Y}_n \overset{\text{p}}{\to} c$, onde $c \in \mathbb{R}^k$ é constante, então $(\boldsymbol{X}_n, \boldsymbol{Y}_n) \overset{d}{\to} (\boldsymbol{X}, c)$.
				
				\item Se $\boldsymbol{X}_n \overset{\text{p}}{\to} \boldsymbol{X}$ e $\boldsymbol{Y}_n \overset{\text{p}}{\to} \boldsymbol{Y}$, então $(\boldsymbol{X}_n, \boldsymbol{Y}_n) \overset{\text{p}}{\to} (\boldsymbol{X}, \boldsymbol{Y}) $.
			\end{enumerate}
		\end{lemma}
		\begin{corollary}[Lema de Slutsky] Sejam
	${X}_n \overset{\text{d}}{\to} {X}$ e ${Y}_n \overset{\text{p}}{\to} c$, duas sequências de variáveis aleatórias ou vetores aleatórios, onde $c $ é constante, então:
	
	\begin{itemize}
		\item $X_n + Y_n \overset{d}{\to} X + c$.
				\item $X_n \cdot Y_n \overset{d}{\to} X \cdot c$.
				\item Se $c \neq 0$, $X_n/Y_n  \overset{d}{\to} X / c$.
	\end{itemize}
	
		\end{corollary}
	\end{frame}
	
	\begin{frame}{Notação $O$ e $o$ para sequências não estocásticas}
		\begin{itemize}
			\item 	Sejam $(x_n)_n$ e $(y_n)_n$ duas sequências de \textbf{números reais}.
		\item	Dizemos que $x_n = o(y_n)$ se $\lim_{n \to \infty} \frac{x_n}{y_n} = 0$.
		\item 	Dizemos que $x_n = O(y_n)$ se existem $C > 0$ e $K \in \mathbb{N}$ tais que:
		$\left|x_n\right| \leq C | y_n|$, para todo $n \geq K$.
		\begin{itemize}
			\item Nesse caso, a razão $\frac{x_n}{y_n}$ é limitada.
		\end{itemize}
		\end{itemize}

	\end{frame}
	
		\begin{frame}{Notação $O_p$ e $o_p$ para sequências estocásticas}
		\begin{itemize}
			\item Sejam $X_n$ e $Y_n$ sequências de {\color{red}variáveis aleatórias}. 
			\item Dizemos que $X_n = o_p(Y_n)$ se $\frac{X_n}{Y_n} \overset{p}{\to} 0$.
			\item Dizemos que $X_n = O_p(Y_n)$ se, para todo $\epsilon > 0$, existem $M > 0$ e $K \in \mathbb{N}$ tais que:
			$$\mathbb{P}[|X_n|> M |Y_n|] \leq \epsilon\, , \forall n \geq K\, .$$
			\begin{itemize}
				\item Nesse caso, dizemos que $\left|\frac{X_n}{Y_n}\right|$ é {\color{magenta}limitada  em probabilidade}.
			\end{itemize}			
				\end{itemize}
	\end{frame}
	\begin{frame}{Notação $O_p$ e $o_p$: propriedades}
		\begin{lemma}
		\begin{itemize}
			\item $o_p(1) + o_p(1) = o_p(1)$.
			\item $o_p(1) + O_p(1) = O_p(1)$.
			\item $O_p(1) o_p(1) = o_p(1)$.
			\item $\frac{1}{(1+o_p(1))} = O_P(1)$
			\item $o_p(Y_n) = Y_n o_p(1)$. 
			\item $O_p(Y_n) = Y_n O_p(1)$. 
			\item $o_p(O_p(1)) = o_p(1)$.
			\item Se $X_n$ converge em distribuição, $X_n = O_p(1)$.
			\item Seja $\phi: \mathbb{R}^k \mapsto \mathbb{R}$ uma função tal que $\phi(\boldsymbol{0}) = 0$. Se $X_n \overset{p}{\to} \boldsymbol{0}$, então, para qualquer $p > 0$:
			\begin{enumerate}
				\item[a] Se $R(h) = o(\lVert h \rVert^p)$ quando $h \to 0$, então $R(\boldsymbol{X_n}) = o_p(\lVert \boldsymbol{X_n} \rVert^p)$.
				\item[b] Se $R(h) = O(\lVert h \rVert^p)$ quando $h \to 0$, então $R(\boldsymbol{X_n}) = O_p(\lVert \boldsymbol{X_n} \rVert^p)$.
			\end{enumerate}
		\end{itemize}
	\end{lemma}
	\end{frame}
	
	\begin{frame}{Método Delta}

	\begin{itemize}
		\item Seja $\boldsymbol{X}_n$ uma sequência de vetores aleatórios. 
		\item Suponha que existe uma sequência de números reais $r_n \to \infty$, uma constante $\theta \in \mathbb{R}^k$ e um vetor aleatório $\boldsymbol{S}$ tais que: $r_n(\boldsymbol{X}_n-\theta) \overset{d}{\to}\boldsymbol{S}$.
		\begin{itemize}
			\item Nesse caso, $\boldsymbol{X}_n \overset{p}{\to} \theta$ (por quê?).
		\end{itemize}
		\item Será que podemos calcular $r_n(\psi(\boldsymbol{X}_n)-\psi(\theta))$ para uma transformação $\psi:\mathbb{R}^k \mapsto \mathbb{R}^l$?
		\begin{itemize}
			\item Interpretação estatística: derivar a distribuição em amostras grandes de uma estatística a partir de outra estatística.
		\end{itemize}
	\end{itemize}
	\begin{lemma}[Método Delta]
	Se $\psi$ é continuamente diferenciável numa vizinhança de $\theta$, então:
	$$r_n(\psi(\boldsymbol{X}_n)-\psi(\theta)) \overset{d}{\to} \nabla_{x}\psi(\theta) S\, ,$$
	onde $\nabla_{x}\psi(\theta)$ é o Jacobiano de $\psi$ avaliado em $\theta$
	\end{lemma}
	\end{frame}
	\begin{frame}{Função geradora de momentos}
		\begin{itemize}
			\item Seja $X$ uma variável aleatória real. Definimos a função geradora de momentos, $M: \mathcal{S} \mapsto \mathbb{R}_+$, $\mathcal{S}\subseteq \mathbb{R}$, como a função dada por.
			
			$$M(s) = \mathbb{E}[\exp(sX)]\, ,\quad  s \in \mathcal{S}\, ,$$
			onde  $\mathcal{S}$ é o conjunto de pontos em que a esperança $ \mathbb{E}[\exp(sX)]$ é finita.

			
			\begin{proposition}
				Se existe $\epsilon > 0$ tal que $(-\epsilon,\epsilon)\subseteq\mathcal{S}$, então $X$ possui todos os momentos (i.e. $\mathbb{E}[|X]^j] < \infty$ para todo $j \in \mathbb{N}$), a expansão de Taylor infinita de $M(s)$ em torno de $0$ é exata para $|s|<\epsilon$, i.e.
				$$M(s) = \sum_{j=0}^\infty  M^{(j)}(0) \frac{s^j}{j!} \,,$$
				e as derivadas em zero podem ser calculadas por diferenciação sob a integral, resultando em $M^{(j)}(0) = \mathbb{E}[X^j]$ para todo $j \in \mathbb{N}$.
			\end{proposition}
		\end{itemize}
	\end{frame}
	\begin{frame}{Função característica}
		\begin{itemize}
			\item Dificuldade de utilizar função característica é que nem sempre ela está disponível numa vizinhança do zero.
			\item Por esse motivo, definimos uma função alternativa, conhecida como {\color{red}função característica}. 
			\item Seja $\boldsymbol{X}$ um vetor aleatório de dimensão $k$. Sua função característica é dada por $\phi: \mathbb{R}^k \mapsto \mathbb{C}$, definida por:
			\begin{equation*}
				\phi(s) = \mathbb{E}[e^{is'\boldsymbol{X}}] = \mathbb{E}[\cos(s'\boldsymbol{X})] + i \mathbb{E}[\sin(s'\boldsymbol{X})]
			\end{equation*} 
			\item Função está sempre bem-definida, para todo $s \in \mathbb{R}^k$ (por quê?).
			\item {\color{blue}Propriedade útil 1:} se $\boldsymbol{X}$ é escalar ($k=1$), e $\mathbb{E}[|\boldsymbol{X}|^j]<\infty$ para $j \in \mathbb{N}$ então $\phi$ é $j$-vezes diferenciável no zero, com $j$-ésima derivada dada por diferenciação sob a integral, resultando em $\phi^{(j)}(0) = i^j \mathbb{E}[\boldsymbol{X}^j]$.
			\item {\color{blue}Propriedade útil 2:} se $\boldsymbol{X}$ e $\boldsymbol{Y}$ são vetores aleatórios independentes, $\mathbb{E}[\exp(is'(\boldsymbol{X}+\boldsymbol{Y}))] = \mathbb{E}[\exp(is'\boldsymbol{X})] \mathbb{E}[\exp(is'\boldsymbol{Y})]$. 
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Caracterizações com base em funções característica}
		\begin{itemize}
			\item Funções características apresentam propriedades poderosas de caracterização de distribuições, que coletamos (sem demonstrar) abaixo.
		\end{itemize}
		\begin{proposition}
		\begin{enumerate}
			\item Dois vetores aleatórios, $\boldsymbol{X}$ e $\boldsymbol{Y}$, possuem funções distribuição iguais se, e somente se, suas funções características são idênticas.
			\item Uma sequência de vetores aleatórios $\boldsymbol{X}_n$ convergem em distribuição para um vetor aleatório $\boldsymbol{Y}$ se, e somente se, as funções características de $\boldsymbol{X}_n$ convergem para a função característica de $\boldsymbol{Y}$ pontualmente em todo $s \in \mathbb{R}^k$, i.e. $\phi_{\boldsymbol{X}_n}(s) \to \phi_{\boldsymbol{Y}}(s)$ para todo $s \in \mathbb{R}^k$. 
		\end{enumerate}
		\end{proposition}
		\begin{corollary}[Dispositivo de Crámer-Wold]
			$\boldsymbol{X}_n \overset{d}{\to} \boldsymbol{X}$ se, e somente se, $t'\boldsymbol{X}_n  \overset{d}{\to} t'\boldsymbol{X}$ para todo $t \in \mathbb{R}^k$.
		\end{corollary}
	\end{frame}
	
	\begin{frame}{Lei forte dos grandes números}
		\begin{itemize}
			\item Dizemos que uma sequência de vetores aleatórios $\boldsymbol{X}_1, \boldsymbol{X}_2, \ldots$ é independente e identicamente distribuída (iid) se os vetores aleatórios são independentes (i.e. suas $\sigma$-álgebras geradas são independentes) e se suas funções distribuição $F_{\boldsymbol{X}_j}$ coincidem.
			\begin{itemize}
				\item Pelo lema do $\pi$-sistema, a lei induzida por cada $\boldsymbol{X}_j$ sobre $\mathcal{B}(\mathbb{R}^k)$ é igual.
			\end{itemize}
		\end{itemize}
	\begin{proposition}[Lei forte de Kolmogorov]
			Seja $X_1, X_2,\ldots$ uma sequência de variáveis aleatórias iid em $L^1(\Omega,\Sigma,\mathbb{P})$.
			Então existe $\mu \in \mathbb{R}$ tal que $\mathbb{E}[X_j] = \mu$ para todo $j\in \mathbb{N}$ e, quando $n \to \infty$:
$$\frac{1}{n}\sum_{j=1}^n X_j \overset{\text{q.c.}}{\to} \mu\, .$$
	\end{proposition}

	\end{frame}
	
		\begin{frame}{Lei fraca dos grandes números}
			\begin{corollary}[Lei fraca de Khinchine]
			Seja $X_1, X_2,\ldots$ uma sequência de variáveis aleatórias iid em $L^1(\Omega,\Sigma,\mathbb{P})$. Então temos quando $n \to \infty$:
			
			$$\frac{1}{n}\sum_{j=1}^n X_j \overset{\text{p}}{\to} \mathbb{E}[X_1]\, .$$
		\end{corollary}
		
					\begin{proposition}[Lei fraca de Markov]
			Seja $X_1, X_2,\ldots$ uma sequência de variáveis aleatórias em $L^{\color{red}2}(\Omega,\Sigma,\mathbb{P})$, não correlacionadas e tais que $\sup_{j \in \mathbb{N}}\mathbb{V}[X_j] <\infty$. Se $\lim_{n\to\infty} \frac{1}{n}\sum_{j=1}^n \mathbb{E}[X_j]$ existe em $\mathbb{R}$, então, denotando esse limite por $\mu$, temos, quando $n \to \infty$:
			 
		$$\frac{1}{n}\sum_{j=1}^n X_j \overset{\text{p}}{\to} \mu\, .$$
		\end{proposition}
	\end{frame}
		\begin{frame}{Teorema Central do Limite}
			\begin{proposition}
						Seja $X_1, X_2,\ldots$ uma sequência de variáveis aleatórias iid em $L^2(\Omega,\Sigma,\mathbb{P})$. Então, denotando por $\mu = \mathbb{E}[X_1]$ e $\sigma^2 = \mathbb{V}[X_1]$, temos, quando $n \to \infty$:
						
						
						$$\frac{1}{\sqrt{n}}\sum_{j=1}^n (X_1 - \mu) \overset{d}{\to} \mathcal{N}(0,\sigma^2)\, , $$
						onde $\mathcal{N}(0,\sigma^2)$ denota uma variável aleatória com distribuição normal com média zero e variância $\sigma^2$.
			\end{proposition}

	\end{frame}
\end{document}
